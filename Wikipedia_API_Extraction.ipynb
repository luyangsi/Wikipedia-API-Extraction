{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wikipedia-api sentence-transformers scikit-learn pandas numpy tqdm"
      ],
      "metadata": {
        "id": "-I-_f9avM1aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# ✅ 自动设置支持中文的字体（适用于 Windows / Mac / Linux）\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Microsoft YaHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False   # 解决负号 '-' 显示为方块的问题\n"
      ],
      "metadata": {
        "id": "h59uxYgOmafb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 你的关键词字典（原样粘贴即可） ====\n",
        "KEYWORDS_BY_CATEGORY = {\n",
        "    \"race\": [\n",
        "        \"race\", \"ethnicity\", \"ethnic group\", \"racial identity\", \"racial background\", \"ancestry\",\n",
        "        \"heritage\", \"minority\", \"majority\", \"person of color\", \"BIPOC\", \"POC\", \"nonwhite\", \"white\",\n",
        "        \"Black\", \"African American\", \"African\", \"Caribbean\", \"Asian\", \"East Asian\", \"South Asian\",\n",
        "        \"Southeast Asian\", \"Pacific Islander\", \"Desi\", \"Hispanic\", \"Latino\", \"Latina\", \"Latinx\",\n",
        "        \"Chicano\", \"Mexican American\", \"Puerto Rican\", \"Cuban\", \"Native American\", \"Indigenous\",\n",
        "        \"First Nation\", \"Inuit\", \"Aboriginal\", \"Middle Eastern\", \"Arab\", \"Persian\", \"Iranian\",\n",
        "        \"Turkish\", \"Kurdish\", \"Israeli\", \"Palestinian\", \"racism\", \"racial bias\", \"racial profiling\",\n",
        "        \"colorism\", \"xenophobia\", \"hate crime\", \"racial slur\", \"microaggression\"\n",
        "    ],\n",
        "\n",
        "    \"color\": [\n",
        "        \"skin color\", \"complexion\", \"tone\", \"pigment\", \"light-skinned\", \"dark-skinned\", \"fair\",\n",
        "        \"tan\", \"olive\", \"brown\", \"black\", \"white\", \"color bias\", \"shadeism\", \"colorism\",\n",
        "        \"bleaching\", \"whitening\", \"tanning\", \"beauty standards\"\n",
        "    ],\n",
        "\n",
        "    \"religion\": [\n",
        "        \"religion\", \"belief\", \"faith\", \"spirituality\", \"worship\", \"sect\", \"denomination\", \"atheist\",\n",
        "        \"agnostic\", \"believer\", \"Christian\", \"Catholic\", \"Protestant\", \"Evangelical\", \"Baptist\",\n",
        "        \"Mormon\", \"Orthodox\", \"Muslim\", \"Islam\", \"Sunni\", \"Shia\", \"Sufi\", \"Hijab\", \"Ramadan\",\n",
        "        \"Quran\", \"Jewish\", \"Judaism\", \"Torah\", \"synagogue\", \"kosher\", \"Hanukkah\", \"Hindu\",\n",
        "        \"Hinduism\", \"karma\", \"dharma\", \"temple\", \"yoga\", \"Buddhist\", \"Buddhism\", \"Zen\",\n",
        "        \"meditation\", \"monk\", \"Sikh\", \"Bahá’í\", \"Jain\", \"Shinto\", \"religious discrimination\",\n",
        "        \"Islamophobia\", \"antisemitism\", \"anti-Christian sentiment\", \"blasphemy\", \"religious intolerance\"\n",
        "    ],\n",
        "\n",
        "    \"sex_gender\": [\n",
        "        \"sex\", \"gender\", \"gender identity\", \"gender expression\", \"sexual orientation\",\n",
        "        \"sexuality\", \"reproductive status\", \"male\", \"female\", \"man\", \"woman\", \"boy\", \"girl\",\n",
        "        \"intersex\", \"transgender\", \"trans\", \"trans man\", \"trans woman\", \"nonbinary\", \"genderqueer\",\n",
        "        \"genderfluid\", \"two-spirit\", \"pronoun\", \"misgender\", \"transition\", \"heterosexual\",\n",
        "        \"homosexual\", \"gay\", \"lesbian\", \"bisexual\", \"pansexual\", \"asexual\", \"queer\", \"LGBTQIA+\",\n",
        "        \"same-sex\", \"pregnancy\", \"pregnant\", \"maternity\", \"paternity\", \"childbirth\", \"breastfeeding\",\n",
        "        \"parental leave\", \"miscarriage\", \"fertility\", \"sexism\", \"misogyny\", \"homophobia\",\n",
        "        \"transphobia\", \"heteronormativity\", \"gender bias\", \"sexual harassment\", \"pregnancy discrimination\"\n",
        "    ],\n",
        "\n",
        "    \"national_origin\": [\n",
        "        \"national origin\", \"nationality\", \"citizenship\", \"country of origin\", \"immigration status\",\n",
        "        \"migrant\", \"refugee\", \"asylum seeker\", \"foreigner\", \"alien\", \"expatriate\", \"immigrant\",\n",
        "        \"undocumented\", \"border control\", \"visa\", \"naturalized citizen\", \"deportation\", \"green card\",\n",
        "        \"H-1B\", \"DACA\", \"xenophobia\", \"anti-immigrant\", \"nativism\", \"foreigner bias\",\n",
        "        \"accent discrimination\"\n",
        "    ],\n",
        "\n",
        "    \"age\": [\n",
        "        \"age\", \"aging\", \"older adult\", \"elderly\", \"senior\", \"middle-aged\", \"retirement\", \"lifespan\",\n",
        "        \"generational gap\", \"baby boomer\", \"Gen X\", \"senior citizen\", \"retiree\", \"older worker\",\n",
        "        \"ageism\", \"age discrimination\", \"overqualified\", \"outdated\", \"too old\", \"not tech-savvy\",\n",
        "        \"energetic youth\", \"fresh talent\"\n",
        "    ],\n",
        "\n",
        "    \"disability\": [\n",
        "        \"disability\", \"disabled\", \"differently abled\", \"impairment\", \"accessibility\", \"inclusion\",\n",
        "        \"accommodation\", \"assistive technology\", \"physical disability\", \"mobility impairment\",\n",
        "        \"wheelchair\", \"blind\", \"low vision\", \"deaf\", \"hard of hearing\", \"intellectual disability\",\n",
        "        \"developmental disability\", \"autism\", \"ADHD\", \"dyslexia\", \"learning disability\",\n",
        "        \"mental illness\", \"PTSD\", \"ableism\", \"handicap\", \"disabled bias\", \"stigma\", \"inspirational\",\n",
        "        \"burden\", \"accessibility barrier\"\n",
        "    ],\n",
        "\n",
        "    \"genetic_information\": [\n",
        "        \"genetic information\", \"DNA\", \"gene\", \"genes\", \"genome\", \"hereditary\", \"inherited\",\n",
        "        \"mutation\", \"biomarker\", \"predisposition\", \"genetic testing\", \"family history\",\n",
        "        \"carrier\", \"genotype\", \"phenotype\", \"genetic discrimination\", \"health privacy\",\n",
        "        \"insurance bias\", \"GINA\", \"predictive testing\", \"genetic privacy\"\n",
        "    ],\n",
        "\n",
        "    \"cross_cutting\": [\n",
        "        \"discrimination\", \"harassment\", \"retaliation\", \"protected class\", \"equal opportunity\",\n",
        "        \"affirmative action\", \"Title VII\", \"Civil Rights Act\", \"EEOC\", \"ADA\", \"ADEA\", \"GINA\",\n",
        "        \"Section 504\", \"inclusion\", \"diversity\", \"equity\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ==== 运行与导出参数 ====\n",
        "MAX_PAGES_PER_TERM = 10          # 每个具体term最多抓多少个英文页面\n",
        "MIN_SENT_LEN = 8                # 句子最小长度（字符），过滤噪声\n",
        "TOP_DIFFS_PER_PAGE = 12         # 每页导出“潜在差异”句对条数（最低相似度优先）\n",
        "DEVICE = \"cpu\"                 # \"cuda\" 或 \"cpu\"\n",
        "OUT_DIR = \"wiki_sem_diff_outputs\"  # 输出目录\n"
      ],
      "metadata": {
        "id": "gPTv-iVeLxxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, time, random, json, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "EN_WIKI, ZH_WIKI = \"en\", \"zh\"\n",
        "EMB_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "WIKI_API = \"https://{lang}.wikipedia.org/w/api.php\"\n",
        "SEARCH_SLOTS = 50  # MediaWiki 限制不要超过 50\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    if not os.path.exists(p):\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def safe_name(s: str) -> str:\n",
        "    return re.sub(r'[^a-zA-Z0-9._-]+', '_', s.strip())[:120] or \"item\"\n",
        "\n",
        "# ---------- 403/429 热修复：全局 Session + 自定义 User-Agent + 指数退避重试 + 软限速 ----------\n",
        "WIKI_HEADERS = {\n",
        "    # 请改成你自己的邮箱或主页，便于 Wikimedia 与你联系（这是最佳实践）\n",
        "    \"User-Agent\": \"WikiSemDiff/0.2 (research; contact: your_email@example.com)\",\n",
        "    \"Accept-Language\": \"en-US,zh-CN;q=0.9\"\n",
        "}\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.2,\n",
        "    status_forcelist=[403, 429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "session.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "\n",
        "REQUEST_DELAY_BASE = 0.20   # 每次请求的基础延时（秒）\n",
        "REQUEST_DELAY_JITTER = 0.20 # 抖动（0~0.2s）\n",
        "\n",
        "def _wiki_get(lang: str, params: dict, timeout: int = 30) -> dict:\n",
        "    merged = {\n",
        "        \"origin\": \"*\",\n",
        "        \"format\": \"json\",\n",
        "        \"formatversion\": 2,\n",
        "        **params\n",
        "    }\n",
        "    time.sleep(REQUEST_DELAY_BASE + random.random() * REQUEST_DELAY_JITTER)\n",
        "    r = session.get(f\"https://{lang}.wikipedia.org/w/api.php\",\n",
        "                    params=merged, headers=WIKI_HEADERS, timeout=timeout)\n",
        "    # 若仍遇到 403/429，稍作等待后由 Retry 继续处理\n",
        "    if r.status_code in (403, 429):\n",
        "        time.sleep(1.5 + random.random())\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def mediawiki_search(keyword: str, lang: str = EN_WIKI, limit: int = 10) -> List[Dict]:\n",
        "    results, left, sroffset = [], limit, 0\n",
        "    while left > 0:\n",
        "        ask = min(left, SEARCH_SLOTS)\n",
        "        data = _wiki_get(lang, {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": keyword,\n",
        "            \"srlimit\": ask,\n",
        "            \"sroffset\": sroffset,\n",
        "        })\n",
        "        batch = (data.get(\"query\") or {}).get(\"search\") or []\n",
        "        results.extend(batch)\n",
        "        left -= len(batch)\n",
        "        if len(batch) < ask:\n",
        "            break\n",
        "        sroffset += len(batch)\n",
        "    # 去重仅保留 title/pageid\n",
        "    out, seen = [], set()\n",
        "    for it in results:\n",
        "        title, pageid = it.get(\"title\"), it.get(\"pageid\")\n",
        "        if title and pageid and pageid not in seen:\n",
        "            out.append({\"title\": title, \"pageid\": pageid})\n",
        "            seen.add(pageid)\n",
        "    return out[:limit]\n",
        "\n",
        "def get_langlink(pageid: int, target_lang: str = ZH_WIKI, source_lang: str = EN_WIKI) -> Optional[Dict]:\n",
        "    data = _wiki_get(source_lang, {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"langlinks\",\n",
        "        \"pageids\": pageid,\n",
        "        \"lllang\": target_lang,\n",
        "    })\n",
        "    pages = (data.get(\"query\") or {}).get(\"pages\") or []\n",
        "    if pages:\n",
        "        ll = pages[0].get(\"langlinks\") or []\n",
        "        if ll:\n",
        "            return {\"lang\": ll[0].get(\"lang\"), \"title\": ll[0].get(\"title\")}\n",
        "    return None\n",
        "\n",
        "def get_plain_text(title: str, lang: str) -> str:\n",
        "    data = _wiki_get(lang, {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": 1,\n",
        "        \"titles\": title,\n",
        "    })\n",
        "    pages = (data.get(\"query\") or {}).get(\"pages\") or []\n",
        "    texts = []\n",
        "    for p in pages:\n",
        "        ext = p.get(\"extract\")\n",
        "        if ext:\n",
        "            texts.append(ext)\n",
        "    return \"\\n\".join(texts).strip()\n",
        "\n",
        "# ---------------- 文本清洗与句子切分 ----------------\n",
        "def clean_text(t: str) -> str:\n",
        "    t = re.sub(r'\\[[0-9]+\\]', '', t)\n",
        "    t = re.sub(r'[ \\t]+', ' ', t)\n",
        "    t = re.sub(r'\\n{2,}', '\\n', t)\n",
        "    return t.strip()\n",
        "\n",
        "def sent_split_en(text: str) -> List[str]:\n",
        "    text = text.replace('\\n', ' ')\n",
        "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s.strip() for s in sents if len(s.strip()) >= MIN_SENT_LEN]\n",
        "\n",
        "def sent_split_zh(text: str) -> List[str]:\n",
        "    text = text.replace('\\n', '')\n",
        "    sents = re.split(r'(?<=[。！？])', text)\n",
        "    return [s.strip() for s in sents if len(s.strip()) >= MIN_SENT_LEN]\n",
        "\n",
        "# ---------------- 多语句嵌入对齐 ----------------\n",
        "def pairwise_align(en_sents: List[str], zh_sents: List[str],\n",
        "                   model: SentenceTransformer, device: str = \"cpu\") -> pd.DataFrame:\n",
        "    if not en_sents or not zh_sents:\n",
        "        return pd.DataFrame(columns=[\"en_sentence\", \"zh_best\", \"cosine\", \"zh_idx\"])\n",
        "    en_emb = model.encode(en_sents, convert_to_tensor=True, device=device, normalize_embeddings=True)\n",
        "    zh_emb = model.encode(zh_sents, convert_to_tensor=True, device=device, normalize_embeddings=True)\n",
        "    cos = util.cos_sim(en_emb, zh_emb).cpu().numpy()\n",
        "    best_idx = np.argmax(cos, axis=1)\n",
        "    best_val = cos[np.arange(len(en_sents)), best_idx]\n",
        "    df = pd.DataFrame({\n",
        "        \"en_sentence\": en_sents,\n",
        "        \"zh_idx\": best_idx,\n",
        "        \"cosine\": best_val\n",
        "    })\n",
        "    df[\"zh_best\"] = df[\"zh_idx\"].apply(lambda i: zh_sents[i] if 0 <= i < len(zh_sents) else \"\")\n",
        "    return df[[\"en_sentence\", \"zh_best\", \"cosine\", \"zh_idx\"]].sort_values(\"cosine\", ascending=True).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "cqCEQGRkOJic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(EMB_MODEL_NAME, device=DEVICE)\n",
        "print(\"Loaded:\", EMB_MODEL_NAME, \"on\", DEVICE)\n",
        "ensure_dir(OUT_DIR)\n"
      ],
      "metadata": {
        "id": "pnujLUq5OLb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_keywords_by_category(\n",
        "        keywords_by_category: Dict[str, List[str]],\n",
        "        max_pages_per_term: int,\n",
        "        top_diffs_per_page: int,\n",
        "        device: str = \"cpu\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    对字典 {category: [term1, term2, ...]} 批量运行。\n",
        "    返回：\n",
        "      - diffs_all_df: 汇总所有类别/term 的“低相似度句对”（潜在语义差异）\n",
        "      - summary_all_df: 汇总每个 en↔zh 页面对的整体相似度统计\n",
        "    \"\"\"\n",
        "    all_diffs, all_summary = [], []\n",
        "    processed_pairs = set()  # 去重：(en_title, zh_title)\n",
        "\n",
        "    for cate, terms in keywords_by_category.items():\n",
        "        print(f\"=== Category: {cate} | {len(terms)} terms ===\")\n",
        "        for term in tqdm(terms, desc=f\"[{cate}] terms\"):\n",
        "            # 用 term 在英文维基检索页面\n",
        "            search_res = mediawiki_search(term, EN_WIKI, limit=max_pages_per_term)\n",
        "            for item in search_res:\n",
        "                en_title, en_pageid = item[\"title\"], item[\"pageid\"]\n",
        "                zh_link = get_langlink(en_pageid, target_lang=ZH_WIKI, source_lang=EN_WIKI)\n",
        "                if not zh_link:\n",
        "                    continue\n",
        "                zh_title = zh_link[\"title\"]\n",
        "                key = (en_title, zh_title)\n",
        "                if key in processed_pairs:\n",
        "                    continue\n",
        "                processed_pairs.add(key)\n",
        "\n",
        "                try:\n",
        "                    en_text = clean_text(get_plain_text(en_title, EN_WIKI))\n",
        "                    zh_text = clean_text(get_plain_text(zh_title, ZH_WIKI))\n",
        "                except Exception as e:\n",
        "                    print(f\"[Skip] fetch text error for {en_title} -> {zh_title}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                en_sents = sent_split_en(en_text)\n",
        "                zh_sents = sent_split_zh(zh_text)\n",
        "                if not en_sents or not zh_sents:\n",
        "                    continue\n",
        "\n",
        "                df_align = pairwise_align(en_sents, zh_sents, model=model, device=device)\n",
        "                sim_vals = df_align[\"cosine\"].values\n",
        "\n",
        "                # summary 行\n",
        "                all_summary.append({\n",
        "                    \"category\": cate,\n",
        "                    \"term\": term,\n",
        "                    \"en_title\": en_title,\n",
        "                    \"zh_title\": zh_title,\n",
        "                    \"n_en_sent\": len(en_sents),\n",
        "                    \"n_zh_sent\": len(zh_sents),\n",
        "                    \"sim_mean\": float(np.mean(sim_vals)),\n",
        "                    \"sim_median\": float(np.median(sim_vals)),\n",
        "                    \"sim_q25\": float(np.quantile(sim_vals, 0.25)),\n",
        "                    \"sim_q10\": float(np.quantile(sim_vals, 0.10)),\n",
        "                })\n",
        "\n",
        "                # 导出每个页面里相似度最低的若干句对\n",
        "                df_topdiff = df_align.head(top_diffs_per_page).copy()\n",
        "                df_topdiff.insert(0, \"category\", cate)\n",
        "                df_topdiff.insert(1, \"term\", term)\n",
        "                df_topdiff.insert(2, \"en_title\", en_title)\n",
        "                df_topdiff.insert(3, \"zh_title\", zh_title)\n",
        "                all_diffs.append(df_topdiff)\n",
        "\n",
        "                # 软限速，进一步降低被限流概率\n",
        "                time.sleep(0.06)\n",
        "\n",
        "    diffs_all_df = (pd.concat(all_diffs, ignore_index=True)\n",
        "                    if all_diffs else\n",
        "                    pd.DataFrame(columns=[\"category\",\"term\",\"en_title\",\"zh_title\",\"en_sentence\",\"zh_best\",\"cosine\",\"zh_idx\"]))\n",
        "    summary_all_df = pd.DataFrame(all_summary, columns=[\n",
        "        \"category\",\"term\",\"en_title\",\"zh_title\",\"n_en_sent\",\"n_zh_sent\",\n",
        "        \"sim_mean\",\"sim_median\",\"sim_q25\",\"sim_q10\"\n",
        "    ])\n",
        "    return diffs_all_df, summary_all_df\n"
      ],
      "metadata": {
        "id": "qzbX_4TVONBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffs_all, summary_all = analyze_keywords_by_category(\n",
        "    KEYWORDS_BY_CATEGORY,\n",
        "    max_pages_per_term=MAX_PAGES_PER_TERM,\n",
        "    top_diffs_per_page=TOP_DIFFS_PER_PAGE,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "print(\"完成：页面对数量 =\", summary_all.shape[0])\n",
        "display(summary_all.head(20))\n",
        "display(diffs_all.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_oXA4FtOQNM",
        "outputId": "a8e3ff26-6a56-4e7d-f996-10e65e44e40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[race] terms:   2%|▏         | 1/53 [01:53<1:38:07, 113.23s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensure_dir(OUT_DIR)\n",
        "\n",
        "sum_path = os.path.join(OUT_DIR, \"summary_all.csv\")\n",
        "dif_path = os.path.join(OUT_DIR, \"diffs_all.csv\")\n",
        "summary_all.to_csv(sum_path, index=False, encoding=\"utf-8\")\n",
        "diffs_all.to_csv(dif_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Saved:\", sum_path, \"and\", dif_path)\n",
        "\n",
        "# 按类别分别导出，便于分工审阅\n",
        "for cate in sorted(KEYWORDS_BY_CATEGORY.keys()):\n",
        "    sub_sum = summary_all[summary_all[\"category\"] == cate].reset_index(drop=True)\n",
        "    sub_dif = diffs_all[diffs_all[\"category\"] == cate].reset_index(drop=True)\n",
        "    if not sub_sum.empty:\n",
        "        sub_sum.to_csv(os.path.join(OUT_DIR, f\"summary__{safe_name(cate)}.csv\"), index=False, encoding=\"utf-8\")\n",
        "    if not sub_dif.empty:\n",
        "        sub_dif.to_csv(os.path.join(OUT_DIR, f\"diffs__{safe_name(cate)}.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Per-category CSVs saved in:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "D6mh-ozoOSqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_K_GLOBAL = 300\n",
        "SIM_THRESHOLD = 0.5  # 越低越可能存在“语义差异/未对齐”\n",
        "\n",
        "if not diffs_all.empty:\n",
        "    topk = diffs_all.sort_values(\"cosine\").head(TOP_K_GLOBAL).reset_index(drop=True)\n",
        "    thr  = diffs_all[diffs_all[\"cosine\"] <= SIM_THRESHOLD].sort_values(\"cosine\").reset_index(drop=True)\n",
        "\n",
        "    topk_path = os.path.join(OUT_DIR, f\"global_top{TOP_K_GLOBAL}.csv\")\n",
        "    thr_path  = os.path.join(OUT_DIR, f\"global_thr{SIM_THRESHOLD}.csv\")\n",
        "    topk.to_csv(topk_path, index=False, encoding=\"utf-8\")\n",
        "    thr.to_csv(thr_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"Saved:\", topk_path, \"and\", thr_path)\n",
        "else:\n",
        "    print(\"diffs_all is empty.\")\n"
      ],
      "metadata": {
        "id": "uLEPdPLrOT0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84303498"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the output directory\n",
        "output_filename = 'wiki_sem_diff_outputs.zip'\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', OUT_DIR)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(output_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------- load data (current dir or default folder) ----------\n",
        "if os.path.exists(\"summary_all.csv\") and os.path.exists(\"diffs_all.csv\"):\n",
        "    summary_df = pd.read_csv(\"summary_all.csv\")\n",
        "    diffs_df  = pd.read_csv(\"diffs_all.csv\")\n",
        "elif os.path.exists(\"wiki_sem_diff_outputs/summary_all.csv\") and os.path.exists(\"wiki_sem_diff_outputs/diffs_all.csv\"):\n",
        "    summary_df = pd.read_csv(\"wiki_sem_diff_outputs/summary_all.csv\")\n",
        "    diffs_df  = pd.read_csv(\"wiki_sem_diff_outputs/diffs_all.csv\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"summary_all.csv and diffs_all.csv not found in current dir or wiki_sem_diff_outputs/\")\n",
        "\n",
        "# ---------- 1) Category average similarity (bar) ----------\n",
        "cat_stats = summary_df.groupby(\"category\")[\"sim_mean\"].mean().sort_values()\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(cat_stats.index, cat_stats.values)\n",
        "plt.title(\"Average semantic similarity by category\")\n",
        "plt.ylabel(\"Mean cosine similarity\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.show()\n",
        "\n",
        "# ---------- 2) Page-level mean similarity distribution (hist) ----------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(summary_df[\"sim_mean\"].dropna(), bins=30)\n",
        "plt.title(\"Distribution of mean similarities across page pairs\")\n",
        "plt.xlabel(\"Mean cosine similarity\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# ---------- 3) Category boxplot of mean similarity ----------\n",
        "ordered_cats = list(cat_stats.index)\n",
        "data_by_cat = [summary_df.loc[summary_df[\"category\"] == c, \"sim_mean\"].dropna() for c in ordered_cats]\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.boxplot(data_by_cat, labels=ordered_cats, showfliers=False)\n",
        "plt.title(\"Mean similarity by category (boxplot)\")\n",
        "plt.ylabel(\"Mean cosine similarity\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.show()\n",
        "\n",
        "# ---------- 4) Top-10 lowest-similarity pages (barh, EN titles only) ----------\n",
        "worst_pages = summary_df.nsmallest(10, \"sim_mean\").copy()\n",
        "labels_en = [row[\"en_title\"] for _, row in worst_pages.iterrows()]  # use ONLY English titles\n",
        "plt.figure(figsize=(10, 6))\n",
        "y = np.arange(len(worst_pages))\n",
        "plt.barh(y, worst_pages[\"sim_mean\"].values)\n",
        "plt.yticks(y, labels_en)\n",
        "plt.xlabel(\"Mean cosine similarity\")\n",
        "plt.title(\"Top-10 pages with lowest English–Chinese semantic similarity\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# ---------- 5) Sentence-level cosine distribution for low-sim pairs ----------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(diffs_df[\"cosine\"].dropna(), bins=30)\n",
        "plt.title(\"Distribution of cosine similarity (sentence-level mismatches)\")\n",
        "plt.xlabel(\"Cosine similarity\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nKidxbYlnXto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Auto detect CSV location\n",
        "if os.path.exists(\"summary_all.csv\") and os.path.exists(\"diffs_all.csv\"):\n",
        "    summary_df = pd.read_csv(\"summary_all.csv\")\n",
        "    diffs_df = pd.read_csv(\"diffs_all.csv\")\n",
        "elif os.path.exists(\"wiki_sem_diff_outputs/summary_all.csv\"):\n",
        "    summary_df = pd.read_csv(\"wiki_sem_diff_outputs/summary_all.csv\")\n",
        "    diffs_df = pd.read_csv(\"wiki_sem_diff_outputs/diffs_all.csv\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"summary_all.csv 和 diffs_all.csv 未找到，请确认文件路径\")\n",
        "\n",
        "# Threshold for low semantic similarity\n",
        "LOW_THRESHOLD = 0.5\n",
        "\n",
        "# ✅ 1. Total page pairs & sentence pairs\n",
        "total_pages = len(summary_df)\n",
        "total_sent_pairs = len(diffs_df)\n",
        "\n",
        "# ✅ 2. Low-quality sentence pairs count\n",
        "low_quality_count = (diffs_df[\"cosine\"] < LOW_THRESHOLD).sum()\n",
        "low_quality_pct = low_quality_count / total_sent_pairs * 100 if total_sent_pairs else 0\n",
        "\n",
        "# ✅ 3. Replace mean with median at page level\n",
        "#    summary_df 本身包含 sim_median 字段，我们直接基于此判断\n",
        "worst_page = summary_df.nsmallest(1, \"sim_median\").iloc[0]\n",
        "best_page = summary_df.nlargest(1, \"sim_median\").iloc[0]\n",
        "\n",
        "# ✅ 4. Category-level median similarity\n",
        "cat_median_stats = summary_df.groupby(\"category\")[\"sim_median\"].median().sort_values()\n",
        "worst_cat = cat_median_stats.index[0]\n",
        "best_cat = cat_median_stats.index[-1]\n",
        "\n",
        "# ✅ 5. Print new median-based summary\n",
        "print(\"===== Summary Statistics (Using Median Similarity) =====\")\n",
        "print(f\"Total English–Chinese page pairs analyzed         : {total_pages}\")\n",
        "print(f\"Total sentence pairs aligned                      : {total_sent_pairs}\")\n",
        "print(f\"Low-quality sentence pairs (cosine < {LOW_THRESHOLD}) : {low_quality_count} ({low_quality_pct:.2f}%)\")\n",
        "\n",
        "print(\"\\n--- Page-level extremes (by median similarity) ---\")\n",
        "print(f\"Lowest similarity page : {worst_page['en_title']} ↔ {worst_page['zh_title']} \"\n",
        "      f\"(median={worst_page['sim_median']:.4f})\")\n",
        "print(f\"Highest similarity page: {best_page['en_title']} ↔ {best_page['zh_title']} \"\n",
        "      f\"(median={best_page['sim_median']:.4f})\")\n",
        "\n",
        "print(\"\\n--- Category-level median similarity ---\")\n",
        "print(cat_median_stats.round(4))\n",
        "print(f\"\\nCategory with largest semantic gap : {worst_cat} (median={cat_median_stats[0]:.4f})\")\n",
        "print(f\"Category with least semantic gap  : {best_cat} (median={cat_median_stats[-1]:.4f})\")\n"
      ],
      "metadata": {
        "id": "y42LssJNoTrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentence-transformers pandas numpy requests tqdm scipy\n"
      ],
      "metadata": {
        "id": "GEjPz_83GMcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 路径设置：从你之前的结果里读取页面清单（en_title, zh_title 等）\n",
        "# 如果在当前目录找不到，就尝试默认输出目录\n",
        "SUMMARY_PATH = \"summary_all.csv\"\n",
        "if not os.path.exists(SUMMARY_PATH):\n",
        "    SUMMARY_PATH = os.path.join(\"wiki_sem_diff_outputs\", \"summary_all.csv\")\n",
        "\n",
        "# 模型配置：LaBSE 更适合跨语言整篇比较；如需更快可换成 MiniLM 多语模型\n",
        "EMB_MODEL = \"sentence-transformers/LaBSE\"   # 也可改为 \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "CHUNK_STRATEGY = \"sentence\"                 # \"sentence\" 或 \"char\" 两种分块策略\n",
        "SENT_PER_CHUNK = 15                         # sentence 策略下，每块句子数（建议 10~25）\n",
        "CHARS_PER_CHUNK = 1800                      # char 策略下，每块最大字符数（建议 1500~2500）\n",
        "\n",
        "MIN_SENT_LEN = 8                            # 最小句长（字符），过滤噪声\n",
        "MAX_PAGES = None                            # None 表示全量，或如 200 做小规模测试\n",
        "SAVE_CSV = True                             # 是否保存结果 CSV\n",
        "OUT_CSV = \"doclevel_similarity.csv\"         # 输出文件名\n"
      ],
      "metadata": {
        "id": "qKw_DFmvGQZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, time, random, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# ---------- 设备自动选择 ----------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ---------- Wikimedia API: UA + Retry + 限速 ----------\n",
        "WIKI_HEADERS = {\n",
        "    \"User-Agent\": \"WikiPageDocSim/0.1 (research; contact: your_email@example.com)\",\n",
        "    \"Accept-Language\": \"en-US,zh-CN;q=0.9\",\n",
        "}\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5, backoff_factor=1.2,\n",
        "    status_forcelist=[403, 429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "session.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "\n",
        "REQUEST_DELAY_BASE = 0.20\n",
        "REQUEST_DELAY_JITTER = 0.20\n",
        "\n",
        "def _wiki_get(lang: str, params: dict, timeout: int = 30) -> dict:\n",
        "    merged = {\"origin\": \"*\", \"format\": \"json\", \"formatversion\": 2, **params}\n",
        "    time.sleep(REQUEST_DELAY_BASE + random.random() * REQUEST_DELAY_JITTER)\n",
        "    r = session.get(f\"https://{lang}.wikipedia.org/w/api.php\",\n",
        "                    params=merged, headers=WIKI_HEADERS, timeout=timeout)\n",
        "    if r.status_code in (403, 429):\n",
        "        time.sleep(1.5 + random.random())\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def get_plain_text(title: str, lang: str) -> str:\n",
        "    \"\"\"抓取维基页面纯文本（可能含摘要与正文，取决于条目结构）。\"\"\"\n",
        "    data = _wiki_get(lang, {\"action\": \"query\", \"prop\": \"extracts\", \"explaintext\": 1, \"titles\": title})\n",
        "    pages = (data.get(\"query\") or {}).get(\"pages\") or []\n",
        "    texts = []\n",
        "    for p in pages:\n",
        "        ext = p.get(\"extract\")\n",
        "        if ext:\n",
        "            texts.append(ext)\n",
        "    return \"\\n\".join(texts).strip()\n",
        "\n",
        "# ---------- 文本预处理 ----------\n",
        "def clean_text(t: str) -> str:\n",
        "    t = re.sub(r'\\[[0-9]+\\]', '', t)     # 去掉参考文献标注 [1]\n",
        "    t = re.sub(r'[ \\t]+', ' ', t)\n",
        "    t = re.sub(r'\\n{2,}', '\\n', t)\n",
        "    return t.strip()\n",
        "\n",
        "def sent_split_en(text: str) -> List[str]:\n",
        "    text = text.replace('\\n', ' ')\n",
        "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s.strip() for s in sents if len(s.strip()) >= MIN_SENT_LEN]\n",
        "\n",
        "def sent_split_zh(text: str) -> List[str]:\n",
        "    text = text.replace('\\n', '')\n",
        "    sents = re.split(r'(?<=[。！？])', text)\n",
        "    return [s.strip() for s in sents if len(s.strip()) >= MIN_SENT_LEN]\n",
        "\n",
        "# ---------- 分块策略 ----------\n",
        "def chunk_by_sentence(sents: List[str], sent_per_chunk: int = 15) -> List[str]:\n",
        "    chunks, buf = [], []\n",
        "    for s in sents:\n",
        "        buf.append(s)\n",
        "        if len(buf) >= sent_per_chunk:\n",
        "            chunks.append(\" \".join(buf))\n",
        "            buf = []\n",
        "    if buf:\n",
        "        chunks.append(\" \".join(buf))\n",
        "    return chunks\n",
        "\n",
        "def chunk_by_char(text: str, max_chars: int = 1800) -> List[str]:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + max_chars, len(text))\n",
        "        # 尽量在句末断开\n",
        "        sub = text[start:end]\n",
        "        cut = max(sub.rfind(\"。\"), sub.rfind(\".\"), sub.rfind(\"!\"), sub.rfind(\"?\"))\n",
        "        if cut != -1 and start + cut + 1 < end:\n",
        "            end = start + cut + 1\n",
        "        chunks.append(text[start:end].strip())\n",
        "        start = end\n",
        "    return [c for c in chunks if len(c) >= MIN_SENT_LEN]\n",
        "\n",
        "def build_doc_chunks(text: str, lang: str) -> List[str]:\n",
        "    if CHUNK_STRATEGY == \"sentence\":\n",
        "        sents = sent_split_en(text) if lang == \"en\" else sent_split_zh(text)\n",
        "        return chunk_by_sentence(sents, SENT_PER_CHUNK)\n",
        "    else:\n",
        "        return chunk_by_char(text, CHARS_PER_CHUNK)\n",
        "\n",
        "# ---------- 文档级嵌入 ----------\n",
        "def doc_embedding(text: str, lang: str, model: SentenceTransformer) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    返回：文档平均向量 (np.ndarray), chunk 数量\n",
        "    \"\"\"\n",
        "    chunks = build_doc_chunks(text, lang)\n",
        "    if len(chunks) == 0:\n",
        "        return np.zeros((model.get_sentence_embedding_dimension(),), dtype=np.float32), 0\n",
        "    emb = model.encode(chunks, convert_to_tensor=True, device=DEVICE, normalize_embeddings=True)\n",
        "    doc_vec = emb.mean(dim=0).detach().cpu().numpy()\n",
        "    return doc_vec, len(chunks)\n"
      ],
      "metadata": {
        "id": "17NVXoxyGSV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取页面清单\n",
        "summary_df = pd.read_csv(SUMMARY_PATH)\n",
        "if MAX_PAGES is not None:\n",
        "    summary_df = summary_df.head(MAX_PAGES).copy()\n",
        "\n",
        "# 仅保留必须字段\n",
        "need_cols = [\"category\", \"term\", \"en_title\", \"zh_title\"]\n",
        "for c in need_cols:\n",
        "    if c not in summary_df.columns:\n",
        "        raise ValueError(f\"Input CSV缺少字段: {c}\")\n",
        "\n",
        "# 加载模型\n",
        "model = SentenceTransformer(EMB_MODEL, device=DEVICE)\n",
        "print(\"Loaded model:\", EMB_MODEL, \"on\", DEVICE)\n",
        "\n",
        "# 逐页面计算“整篇文档向量”及相似度\n",
        "rows = []\n",
        "for _, r in tqdm(summary_df.iterrows(), total=len(summary_df), desc=\"Computing doc-level similarity\"):\n",
        "    cate = r[\"category\"] if \"category\" in r else None\n",
        "    term = r[\"term\"] if \"term\" in r else None\n",
        "    en_title = r[\"en_title\"]\n",
        "    zh_title = r[\"zh_title\"]\n",
        "\n",
        "    try:\n",
        "        en_text = clean_text(get_plain_text(en_title, \"en\"))\n",
        "        zh_text = clean_text(get_plain_text(zh_title, \"zh\"))\n",
        "    except Exception as e:\n",
        "        print(f\"[Skip] fetch error for {en_title} -> {zh_title}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 文档级向量\n",
        "    en_vec, en_chunks = doc_embedding(en_text, \"en\", model)\n",
        "    zh_vec, zh_chunks = doc_embedding(zh_text, \"zh\", model)\n",
        "\n",
        "    # 计算文档级余弦相似度\n",
        "    if np.linalg.norm(en_vec) == 0 or np.linalg.norm(zh_vec) == 0:\n",
        "        doc_sim = np.nan\n",
        "    else:\n",
        "        doc_sim = float(np.dot(en_vec, zh_vec) / (np.linalg.norm(en_vec) * np.linalg.norm(zh_vec)))\n",
        "\n",
        "    rows.append({\n",
        "        \"category\": cate,\n",
        "        \"term\": term,\n",
        "        \"en_title\": en_title,\n",
        "        \"zh_title\": zh_title,\n",
        "        \"len_en_chars\": len(en_text),\n",
        "        \"len_zh_chars\": len(zh_text),\n",
        "        \"n_en_chunks\": en_chunks,\n",
        "        \"n_zh_chunks\": zh_chunks,\n",
        "        \"doc_similarity\": doc_sim,\n",
        "    })\n",
        "\n",
        "docsim_df = pd.DataFrame(rows)\n",
        "print(\"Done. Page pairs computed:\", len(docsim_df))\n",
        "display(docsim_df.head(10))\n",
        "\n",
        "if SAVE_CSV and not docsim_df.empty:\n",
        "    docsim_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "    print(\"Saved:\", OUT_CSV)\n"
      ],
      "metadata": {
        "id": "K2fWZf8FGV76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not docsim_df.empty:\n",
        "    valid = docsim_df.dropna(subset=[\"doc_similarity\"])\n",
        "    print(\"Total page pairs:\", len(docsim_df))\n",
        "    print(\"Valid doc similarities:\", len(valid))\n",
        "    if len(valid):\n",
        "        print(\"Doc-sim median:\", round(valid[\"doc_similarity\"].median(), 4))\n",
        "        print(\"Doc-sim mean  :\", round(valid[\"doc_similarity\"].mean(), 4))\n",
        "        print(\"Doc-sim min   :\", round(valid[\"doc_similarity\"].min(), 4))\n",
        "        print(\"Doc-sim max   :\", round(valid[\"doc_similarity\"].max(), 4))\n",
        "\n",
        "        cat_median = valid.groupby(\"category\")[\"doc_similarity\"].median().sort_values()\n",
        "        print(\"\\nPer-category median doc similarity (ascending):\")\n",
        "        print(cat_median.round(4))\n"
      ],
      "metadata": {
        "id": "ni2ulMzCGYGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}